{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py\n",
    "\n",
    "try:\n",
    "    from gymnasium.wrappers import FrameStackObservation as FrameStack\n",
    "    FS = \"stack_size\"\n",
    "except ImportError:\n",
    "    from gymnasium.wrappers import FrameStack\n",
    "    FS = \"num_stack\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "class LSTMCellUnroller(nn.Module):\n",
    "    def __init__(self, cells: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.cells = cells\n",
    "\n",
    "    @torch.jit.export\n",
    "    def forward(\n",
    "        self,\n",
    "        lstm_in: torch.Tensor, # (B, T, I)\n",
    "        h0: torch.Tensor,    # (L, B, H)\n",
    "        c0: torch.Tensor     # (L, B, H)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, T, _ = lstm_in.shape\n",
    "        L, H    = h0.size(0), h0.size(2)\n",
    "\n",
    "        h = list(h0.unbind(0))\n",
    "        c = list(c0.unbind(0))\n",
    "        out = torch.empty(B, T, H, dtype=lstm_in.dtype, device=lstm_in.device)\n",
    "\n",
    "        for t in range(T):\n",
    "            x = lstm_in[:, t, :]\n",
    "            # TorchScript accepts enumerate over ModuleList\n",
    "            for l, cell in enumerate(self.cells):\n",
    "                h[l], c[l] = cell(x, (h[l], c[l]))\n",
    "                x = h[l] # feed upward\n",
    "            out[:, t, :] = h[-1]\n",
    "\n",
    "        return out, torch.stack(h, 0), torch.stack(c, 0)\n",
    "\n",
    "class RecurrentDuelingDQN(nn.Module):\n",
    "    # R2D2-style recurrent DQN with CNN -> LSTM -> FC architecture.\n",
    "    def __init__(self, input_shape, num_actions, lstm_hidden_size=512, turn_off_lstm=False):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.num_actions = int(num_actions)\n",
    "        self.lstm_hidden_size = int(lstm_hidden_size)\n",
    "        self.turn_off_lstm = turn_off_lstm\n",
    "\n",
    "        # CNN feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            cnn_output_size = self.features(dummy).shape[1]\n",
    "        \n",
    "        if self.turn_off_lstm:\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(cnn_output_size, 512), nn.ReLU(),\n",
    "                nn.Linear(512, 1)\n",
    "            )\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(cnn_output_size, 512), nn.ReLU(),\n",
    "                nn.Linear(512, num_actions)\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # CNN features + one-hot previous action + previous reward\n",
    "        lstm_input_size = int(cnn_output_size + num_actions + 1)\n",
    "        self.num_layers = 1\n",
    "        self.lstm_cells = nn.ModuleList(\n",
    "            [nn.LSTMCell(lstm_input_size, lstm_hidden_size)]\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self._unroller = torch.jit.script(LSTMCellUnroller(self.lstm_cells))\n",
    "\n",
    "    def forward(self, states, prev_actions, prev_rewards, hidden_state=None, out=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: (batch_size, seq_len, C, H, W) or (batch_size, C, H, W)\n",
    "            prev_actions: (batch_size, seq_len) or (batch_size,)\n",
    "            prev_rewards: (batch_size, seq_len) or (batch_size,)\n",
    "            hidden_state: tuple of (h, c) each (1, batch_size, lstm_hidden_size) or None\n",
    "            out: optional output tensor to write results to (avoids allocation)\n",
    "        \"\"\"\n",
    "        if states.dim() == 4: # Single step\n",
    "            states = states.unsqueeze(1)\n",
    "            prev_actions = prev_actions.unsqueeze(1)\n",
    "            prev_rewards = prev_rewards.unsqueeze(1)\n",
    "            single_step = True\n",
    "        else:\n",
    "            single_step = False\n",
    "            \n",
    "        batch_size, seq_len = states.shape[:2]\n",
    "        \n",
    "        # Process through CNN\n",
    "        states_flat = states.reshape(-1, states.size(2), states.size(3), states.size(4)) # (B*T, C, H, W)\n",
    "        cnn_features = self.features(states_flat) # (B*T, cnn_output_size)\n",
    "        cnn_features = cnn_features.view(batch_size, seq_len, -1) # (B, T, cnn_output_size)\n",
    "        \n",
    "        if self.turn_off_lstm:\n",
    "            # Dueling Q-values\n",
    "            values = self.value_stream(cnn_features) # (B, T, 1)\n",
    "            advantages = self.advantage_stream(cnn_features) # (B, T, num_actions)\n",
    "            \n",
    "            # Compute dueling Q-values with optional out parameter\n",
    "            if out is not None:\n",
    "                # Compute advantages - advantages.mean() in-place into out\n",
    "                advantages_mean = advantages.mean(dim=-1, keepdim=True)\n",
    "                torch.sub(advantages, advantages_mean, out=out)\n",
    "                torch.add_(out, values) # Add values in-place\n",
    "                out.copy_(out)\n",
    "                if single_step:\n",
    "                    out.squeeze_(1) # In-place squeeze\n",
    "                q_values = out\n",
    "            else:\n",
    "                # Original allocation-based computation\n",
    "                q_values = (values + (advantages - advantages.mean(dim=-1, keepdim=True)))\n",
    "                if single_step:\n",
    "                    q_values = q_values.squeeze(1) # (B, num_actions)\n",
    "            \n",
    "            return q_values, hidden_state if hidden_state else (torch.zeros((512)), torch.zeros((512)))\n",
    "        \n",
    "        # One-hot encode previous actions\n",
    "        prev_actions_onehot = F.one_hot(prev_actions, self.num_actions).float() # (B, T, num_actions)\n",
    "        \n",
    "        # Prepare previous rewards\n",
    "        prev_rewards = prev_rewards.unsqueeze(-1) # (B, T, 1)\n",
    "\n",
    "        # Concatenate inputs for LSTM\n",
    "        lstm_input = torch.cat([cnn_features, prev_actions_onehot, prev_rewards], dim=-1) # (B, T, lstm_input_size)\n",
    "\n",
    "        # Tensors (L, B, H)\n",
    "        if hidden_state is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size,\n",
    "                            self.lstm_hidden_size, device=states.device)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "        else:\n",
    "            h0, c0 = hidden_state\n",
    "\n",
    "        # Unroll\n",
    "        lstm_out, h_final, c_final = self._unroller(lstm_input, h0, c0)\n",
    "        new_hidden = (h_final, c_final)\n",
    "        \n",
    "        # Dueling Q-values\n",
    "        values = self.value_stream(lstm_out) # (B, T, 1)\n",
    "        advantages = self.advantage_stream(lstm_out) # (B, T, num_actions)\n",
    "\n",
    "        # Compute dueling Q-values into out buffer\n",
    "        if out is not None:\n",
    "            # Check shape compatibility\n",
    "            expected_shape = (batch_size, seq_len, self.num_actions) if not single_step else (batch_size, self.num_actions)\n",
    "            if out.shape != expected_shape:\n",
    "                raise ValueError(f\"out tensor shape {out.shape} doesn't match expected shape {expected_shape}\")\n",
    "            \n",
    "            # Compute advantages - advantages.mean() in-place into out\n",
    "            advantages_mean = advantages.mean(dim=-1, keepdim=True)\n",
    "            torch.sub(advantages, advantages_mean, out=out)\n",
    "            out.add_(values) # Add values in-place\n",
    "            \n",
    "            # Apply value transform in-place\n",
    "            if hasattr(self, '_use_value_transform') and self._use_value_transform:\n",
    "                out.copy_(out)\n",
    "            \n",
    "            if single_step:\n",
    "                out.squeeze_(1) # In-place squeeze\n",
    "            q_values = out\n",
    "        else:\n",
    "            # Calculate into a new buffer\n",
    "            q_values = (values + (advantages - advantages.mean(dim=-1, keepdim=True)))\n",
    "            if single_step:\n",
    "                q_values = q_values.squeeze(1) # (B, num_actions)\n",
    "        \n",
    "        return q_values, new_hidden\n",
    "\n",
    "class RecurrentDQNAgent:\n",
    "    def __init__(self, network: nn.Module, device: torch.device, num_actions: int):\n",
    "        self.network = network.to(device)\n",
    "        self.device = device\n",
    "        self.num_actions = num_actions\n",
    "        self.reset_hidden_state()\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "        self.prev_action = 0 # Start with action 0\n",
    "        self.prev_reward = 0.0\n",
    "\n",
    "    def select_action(self, state_np, override_action=None):\n",
    "        # Select action with optional action override for epsilon-greedy.\n",
    "        state = torch.from_numpy(state_np).unsqueeze(0).float().to(self.device) / 255.0\n",
    "        prev_action = torch.tensor([self.prev_action], dtype=torch.int64, device=self.device)\n",
    "        prev_reward = torch.tensor([self.prev_reward], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values, new_hidden = self.network(state, prev_action, prev_reward, self.hidden_state)\n",
    "            greedy_action = int(q_values.argmax(dim=1).item())\n",
    "            \n",
    "        # Use override action if provided (for epsilon-greedy), otherwise greedy\n",
    "        action = override_action if override_action is not None else greedy_action\n",
    "        \n",
    "        # Always advance hidden state on the observation and set executed action\n",
    "        self.hidden_state = new_hidden\n",
    "        self.prev_action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_prev_reward(self, reward: float):\n",
    "        # Update the previous reward for next action selection\n",
    "        self.prev_reward = reward\n",
    "\n",
    "\n",
    "def _device():\n",
    "    return torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def preview_recurrent_model(\n",
    "    checkpoint_path: str,\n",
    "    num_episodes: int = 5,\n",
    "    render: bool = True,\n",
    "    epsilon: float = 0.0,\n",
    "    game_name: str = 'MsPacman'\n",
    "):\n",
    "    device = _device()\n",
    "    env_name = f\"{game_name}NoFrameskip-v4\"\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "\n",
    "    base_env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
    "    atari_env = AtariPreprocessing(\n",
    "        base_env,\n",
    "        frame_skip=4,\n",
    "        grayscale_obs=True,\n",
    "        scale_obs=False,\n",
    "        noop_max=30,\n",
    "        terminal_on_life_loss=False\n",
    "    )\n",
    "    env = FrameStack(atari_env, **{FS: 4})\n",
    "\n",
    "    obs_shape = env.observation_space.shape\n",
    "    n_actions = env.action_space.n\n",
    "    action_meanings = env.unwrapped.get_action_meanings()\n",
    "    action_counts = {i: 0 for i in range(n_actions)}\n",
    "\n",
    "    print(f\"Observation shape: {obs_shape}, Actions: {n_actions}\")\n",
    "    print(\"Action meanings:\", action_meanings)\n",
    "\n",
    "    net = RecurrentDuelingDQN(obs_shape, n_actions).to(device)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "        env.close()\n",
    "        return\n",
    "\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    agent = RecurrentDQNAgent(net, device, n_actions)\n",
    "\n",
    "    episode_rewards, episode_lengths, episode_lives = [], [], []\n",
    "\n",
    "    obs, _ = env.reset(seed=100500)\n",
    "    s = np.array(obs, dtype=np.uint8)\n",
    "    lives = 3\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "        if lives == 0:\n",
    "            obs, i = env.reset(seed=100500 + ep * 23917)\n",
    "            lives = i['lives']\n",
    "        done = False\n",
    "        total_reward, steps = 0.0, 0\n",
    "\n",
    "        print(f\"\\n=== Episode {ep} ===\")\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "                agent.select_action(s, override_action=a)\n",
    "            else:\n",
    "                a = agent.select_action(s)\n",
    "\n",
    "            action_counts[a] += 1\n",
    "            obs2, r, term, trunc, i = env.step(a)\n",
    "\n",
    "            agent.update_prev_reward(r)\n",
    "            if i[\"lives\"] == 0:\n",
    "                lives = i[\"lives\"]\n",
    "                print(\"Lost last life! Left:\", lives)\n",
    "                agent.reset_hidden_state()\n",
    "\n",
    "            s = np.array(obs2, dtype=np.uint8)\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            done = term or trunc\n",
    "\n",
    "            if steps % 200 == 0:\n",
    "                try:\n",
    "                    print(f\"Steps: {steps}, Lives: {lives}, Reward: {total_reward:.2f}\")\n",
    "                except Exception:\n",
    "                    print(f\"Steps: {steps}, Reward: {total_reward:.2f}\")\n",
    "\n",
    "        final_lives = lives\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        episode_lives.append(final_lives)\n",
    "\n",
    "        print(f\"Episode {ep} finished: Reward={total_reward:.2f}, \"\n",
    "              f\"Lives={final_lives}, Steps={steps}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"SUMMARY OVER {num_episodes} EPISODES\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Avg Reward: {np.mean(episode_rewards):.2f}, \"\n",
    "          f\"Max: {np.max(episode_rewards):.2f}, Min: {np.min(episode_rewards):.2f}\")\n",
    "    print(f\"Avg Steps: {np.mean(episode_lengths):.1f}\")\n",
    "\n",
    "    print(\"\\nAction Usage:\")\n",
    "    total_actions = sum(action_counts.values())\n",
    "    for i, cnt in action_counts.items():\n",
    "        pct = cnt / total_actions * 100 if total_actions else 0.0\n",
    "        print(f\"  {action_meanings[i]}: {cnt} ({pct:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"lengths\": episode_lengths,\n",
    "        \"lives\": episode_lives,\n",
    "        \"action_counts\": action_counts,\n",
    "    }\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=\"MsPacman\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir)\n",
    "             if f.endswith(\".pth\") and pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def find_specific_checkpoint(checkpoint_dir=\"checkpoints\", number=20400, pattern=\"MsPacman\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir)\n",
    "             if f.endswith(\".pth\") and pattern in f and (('ep' + str(number)) in f)]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Ms. Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_pacman/ALE_MsPacman-v5_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_pacman/ALE_MsPacman-v5_best_model.pth\n",
      "Observation shape: (4, 84, 84), Actions: 9\n",
      "Action meanings: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_99466/3473432877.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Steps: 200, Lives: 3, Reward: 3340.00\n",
      "Steps: 400, Lives: 3, Reward: 5230.00\n",
      "Steps: 600, Lives: 3, Reward: 7020.00\n",
      "Steps: 800, Lives: 3, Reward: 10341.00\n",
      "Steps: 1000, Lives: 3, Reward: 10511.00\n",
      "Steps: 1200, Lives: 3, Reward: 10691.00\n",
      "Steps: 1400, Lives: 3, Reward: 14441.00\n",
      "Steps: 1600, Lives: 3, Reward: 16601.00\n",
      "Steps: 1800, Lives: 3, Reward: 17531.00\n",
      "Steps: 2000, Lives: 3, Reward: 17791.00\n",
      "Steps: 2200, Lives: 3, Reward: 17901.00\n",
      "Steps: 2400, Lives: 3, Reward: 18601.00\n",
      "Steps: 2600, Lives: 3, Reward: 19581.00\n",
      "Steps: 2800, Lives: 3, Reward: 19821.00\n",
      "Lost last life! Left: 0\n",
      "Episode 1 finished: Reward=19821.00, Lives=0, Steps=2825\n",
      "\n",
      "========================================\n",
      "SUMMARY OVER 1 EPISODES\n",
      "========================================\n",
      "Avg Reward: 19821.00, Max: 19821.00, Min: 19821.00\n",
      "Avg Steps: 2825.0\n",
      "\n",
      "Action Usage:\n",
      "  NOOP: 42 (1.5%)\n",
      "  UP: 169 (6.0%)\n",
      "  RIGHT: 35 (1.2%)\n",
      "  LEFT: 626 (22.2%)\n",
      "  DOWN: 613 (21.7%)\n",
      "  UPRIGHT: 912 (32.3%)\n",
      "  UPLEFT: 140 (5.0%)\n",
      "  DOWNRIGHT: 272 (9.6%)\n",
      "  DOWNLEFT: 16 (0.6%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [19821.0],\n",
       " 'lengths': [2825],\n",
       " 'lives': [0],\n",
       " 'action_counts': {0: 42,\n",
       "  1: 169,\n",
       "  2: 35,\n",
       "  3: 626,\n",
       "  4: 613,\n",
       "  5: 912,\n",
       "  6: 140,\n",
       "  7: 272,\n",
       "  8: 16}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name = \"MsPacman\"\n",
    "latest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\n",
    "specific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\n",
    "checkpoint = specific or os.path.join(\"checkpoints_pacman\", f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_recurrent_model(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=1,\n",
    "    render=True,\n",
    "    epsilon=0.00,\n",
    "    game_name=game_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_space_invaders/ALE_SpaceInvaders-v5_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_space_invaders/ALE_SpaceInvaders-v5_best_model.pth\n",
      "Observation shape: (4, 84, 84), Actions: 6\n",
      "Action meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_99466/3473432877.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Steps: 200, Lives: 3, Reward: 200.00\n",
      "Steps: 400, Lives: 3, Reward: 465.00\n",
      "Steps: 600, Lives: 3, Reward: 700.00\n",
      "Steps: 800, Lives: 3, Reward: 1025.00\n",
      "Steps: 1000, Lives: 3, Reward: 1270.00\n",
      "Steps: 1200, Lives: 3, Reward: 1590.00\n",
      "Steps: 1400, Lives: 3, Reward: 1905.00\n",
      "Steps: 1600, Lives: 3, Reward: 2295.00\n",
      "Steps: 1800, Lives: 3, Reward: 2595.00\n",
      "Lost last life! Left: 0\n",
      "Episode 1 finished: Reward=2820.00, Lives=0, Steps=1918\n",
      "\n",
      "========================================\n",
      "SUMMARY OVER 1 EPISODES\n",
      "========================================\n",
      "Avg Reward: 2820.00, Max: 2820.00, Min: 2820.00\n",
      "Avg Steps: 1918.0\n",
      "\n",
      "Action Usage:\n",
      "  NOOP: 190 (9.9%)\n",
      "  FIRE: 290 (15.1%)\n",
      "  RIGHT: 274 (14.3%)\n",
      "  LEFT: 252 (13.1%)\n",
      "  RIGHTFIRE: 576 (30.0%)\n",
      "  LEFTFIRE: 336 (17.5%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [2820.0],\n",
       " 'lengths': [1918],\n",
       " 'lives': [0],\n",
       " 'action_counts': {0: 190, 1: 290, 2: 274, 3: 252, 4: 576, 5: 336}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name = \"SpaceInvaders\"\n",
    "latest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\n",
    "specific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\n",
    "checkpoint = specific or os.path.join(\"checkpoints_space_invaders\", f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_recurrent_model(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=1,\n",
    "    render=True,\n",
    "    epsilon=0.00,\n",
    "    game_name=game_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_cartpole/CartPole-v1_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_cartpole/CartPole-v1_best_model.pth\n",
      "Observation dim: 4, Actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_36370/538669789.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=500.00, steps=500\n",
      "Episode 2: reward=500.00, steps=500\n",
      "Episode 3: reward=500.00, steps=500\n",
      "\n",
      "==== SUMMARY ====\n",
      "Avg Reward: 500.00 (max 500.00, min 500.00)\n",
      "Avg Steps: 500.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, policy_net: nn.Module, device: torch.device):\n",
    "        self.policy_net = policy_net.to(device).eval()\n",
    "        self.device = device\n",
    "    def predict(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            q_values = self.policy_net(obs_tensor)\n",
    "            action = int(q_values.argmax(dim=1).item())\n",
    "        return action, None\n",
    "\n",
    "def _device():\n",
    "    return (torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "            else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"))\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"checkpoints_cartpole\", pattern=\"CartPole-v1\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\") and pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def find_specific_checkpoint(checkpoint_dir=\"checkpoints_cartpole\", number=10000, pattern=\"CartPole-v1\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    tag = f\"ep{number}\"\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\") and pattern in f and tag in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def preview_cartpole(\n",
    "    checkpoint_path: str = None,\n",
    "    num_episodes: int = 3,\n",
    "    render: bool = True,\n",
    "    epsilon: float = 0.0\n",
    "):\n",
    "    env_id = \"CartPole-v1\"\n",
    "    device = _device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        latest = find_latest_checkpoint(\"checkpoints_cartpole\", env_id)\n",
    "        best   = os.path.join(\"checkpoints_cartpole\", f\"{env_id}_best_model.pth\")\n",
    "        checkpoint_path = latest if latest is not None else best\n",
    "\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(\"Checkpoint not found.\")\n",
    "        return\n",
    "\n",
    "    env = gym.make(env_id, render_mode=\"human\" if render else None)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    print(f\"Observation dim: {state_dim}, Actions: {action_dim}\")\n",
    "\n",
    "    net = DQN(state_dim, action_dim).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "    agent = DQNAgent(net, device)\n",
    "\n",
    "    episode_rewards, episode_lengths = [], []\n",
    "\n",
    "    for ep in range(1, num_episodes+1):\n",
    "        state, _ = env.reset(seed=100500 + ep)\n",
    "        done, steps, total_reward = False, 0, 0.0\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action, _ = agent.predict(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Episode {ep}: reward={total_reward:.2f}, steps={steps}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\n==== SUMMARY ====\")\n",
    "    print(f\"Avg Reward: {np.mean(episode_rewards):.2f} \"\n",
    "          f\"(max {np.max(episode_rewards):.2f}, min {np.min(episode_rewards):.2f})\")\n",
    "    print(f\"Avg Steps: {np.mean(episode_lengths):.1f}\")\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "specific = None# find_specific_checkpoint(\"checkpoints_cartpole\", number=290, pattern=env_id)\n",
    "latest = None # find_latest_checkpoint(\"checkpoints_cartpole\", pattern=env_id)\n",
    "checkpoint = specific or latest or os.path.join(\"checkpoints_cartpole\", f\"{env_id}_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_cartpole(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=3,\n",
    "    render=True,\n",
    "    epsilon=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
