{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py\n",
    "\n",
    "try:\n",
    "    from gymnasium.wrappers import FrameStackObservation as FrameStack\n",
    "    FS = \"stack_size\"\n",
    "except ImportError:\n",
    "    from gymnasium.wrappers import FrameStack\n",
    "    FS = \"num_stack\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "class LSTMCellUnroller(nn.Module):\n",
    "    def __init__(self, cells: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.cells = cells\n",
    "\n",
    "    @torch.jit.export\n",
    "    def forward(\n",
    "        self,\n",
    "        lstm_in: torch.Tensor, # (B, T, I)\n",
    "        h0: torch.Tensor,    # (L, B, H)\n",
    "        c0: torch.Tensor     # (L, B, H)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, T, _ = lstm_in.shape\n",
    "        L, H    = h0.size(0), h0.size(2)\n",
    "\n",
    "        h = list(h0.unbind(0))\n",
    "        c = list(c0.unbind(0))\n",
    "        out = torch.empty(B, T, H, dtype=lstm_in.dtype, device=lstm_in.device)\n",
    "\n",
    "        for t in range(T):\n",
    "            x = lstm_in[:, t, :]\n",
    "            # TorchScript accepts enumerate over ModuleList\n",
    "            for l, cell in enumerate(self.cells):\n",
    "                h[l], c[l] = cell(x, (h[l], c[l]))\n",
    "                x = h[l] # feed upward\n",
    "            out[:, t, :] = h[-1]\n",
    "\n",
    "        return out, torch.stack(h, 0), torch.stack(c, 0)\n",
    "\n",
    "class RecurrentDuelingDQN(nn.Module):\n",
    "    # R2D2-style recurrent DQN with CNN -> LSTM -> FC architecture.\n",
    "    def __init__(self, input_shape, num_actions, lstm_hidden_size=512, turn_off_lstm=False):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.num_actions = int(num_actions)\n",
    "        self.lstm_hidden_size = int(lstm_hidden_size)\n",
    "        self.turn_off_lstm = turn_off_lstm\n",
    "\n",
    "        # CNN feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            cnn_output_size = self.features(dummy).shape[1]\n",
    "        \n",
    "        if self.turn_off_lstm:\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(cnn_output_size, 512), nn.ReLU(),\n",
    "                nn.Linear(512, 1)\n",
    "            )\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(cnn_output_size, 512), nn.ReLU(),\n",
    "                nn.Linear(512, num_actions)\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # CNN features + one-hot previous action + previous reward\n",
    "        lstm_input_size = int(cnn_output_size + num_actions + 1)\n",
    "        self.num_layers = 1\n",
    "        self.lstm_cells = nn.ModuleList(\n",
    "            [nn.LSTMCell(lstm_input_size, lstm_hidden_size)]\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self._unroller = torch.jit.script(LSTMCellUnroller(self.lstm_cells))\n",
    "\n",
    "    def forward(self, states, prev_actions, prev_rewards, hidden_state=None, out=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: (batch_size, seq_len, C, H, W) or (batch_size, C, H, W)\n",
    "            prev_actions: (batch_size, seq_len) or (batch_size,)\n",
    "            prev_rewards: (batch_size, seq_len) or (batch_size,)\n",
    "            hidden_state: tuple of (h, c) each (1, batch_size, lstm_hidden_size) or None\n",
    "            out: optional output tensor to write results to (avoids allocation)\n",
    "        \"\"\"\n",
    "        if states.dim() == 4: # Single step\n",
    "            states = states.unsqueeze(1)\n",
    "            prev_actions = prev_actions.unsqueeze(1)\n",
    "            prev_rewards = prev_rewards.unsqueeze(1)\n",
    "            single_step = True\n",
    "        else:\n",
    "            single_step = False\n",
    "            \n",
    "        batch_size, seq_len = states.shape[:2]\n",
    "        \n",
    "        # Process through CNN\n",
    "        states_flat = states.reshape(-1, states.size(2), states.size(3), states.size(4)) # (B*T, C, H, W)\n",
    "        cnn_features = self.features(states_flat) # (B*T, cnn_output_size)\n",
    "        cnn_features = cnn_features.view(batch_size, seq_len, -1) # (B, T, cnn_output_size)\n",
    "        \n",
    "        if self.turn_off_lstm:\n",
    "            # Dueling Q-values\n",
    "            values = self.value_stream(cnn_features) # (B, T, 1)\n",
    "            advantages = self.advantage_stream(cnn_features) # (B, T, num_actions)\n",
    "            \n",
    "            # Compute dueling Q-values with optional out parameter\n",
    "            if out is not None:\n",
    "                # Compute advantages - advantages.mean() in-place into out\n",
    "                advantages_mean = advantages.mean(dim=-1, keepdim=True)\n",
    "                torch.sub(advantages, advantages_mean, out=out)\n",
    "                torch.add_(out, values) # Add values in-place\n",
    "                out.copy_(out)\n",
    "                if single_step:\n",
    "                    out.squeeze_(1) # In-place squeeze\n",
    "                q_values = out\n",
    "            else:\n",
    "                # Original allocation-based computation\n",
    "                q_values = (values + (advantages - advantages.mean(dim=-1, keepdim=True)))\n",
    "                if single_step:\n",
    "                    q_values = q_values.squeeze(1) # (B, num_actions)\n",
    "            \n",
    "            return q_values, hidden_state if hidden_state else (torch.zeros((512)), torch.zeros((512)))\n",
    "        \n",
    "        # One-hot encode previous actions\n",
    "        prev_actions_onehot = F.one_hot(prev_actions, self.num_actions).float() # (B, T, num_actions)\n",
    "        \n",
    "        # Prepare previous rewards\n",
    "        prev_rewards = prev_rewards.unsqueeze(-1) # (B, T, 1)\n",
    "\n",
    "        # Concatenate inputs for LSTM\n",
    "        lstm_input = torch.cat([cnn_features, prev_actions_onehot, prev_rewards], dim=-1) # (B, T, lstm_input_size)\n",
    "\n",
    "        # Tensors (L, B, H)\n",
    "        if hidden_state is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size,\n",
    "                            self.lstm_hidden_size, device=states.device)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "        else:\n",
    "            h0, c0 = hidden_state\n",
    "\n",
    "        # Unroll\n",
    "        lstm_out, h_final, c_final = self._unroller(lstm_input, h0, c0)\n",
    "        new_hidden = (h_final, c_final)\n",
    "        \n",
    "        # Dueling Q-values\n",
    "        values = self.value_stream(lstm_out) # (B, T, 1)\n",
    "        advantages = self.advantage_stream(lstm_out) # (B, T, num_actions)\n",
    "\n",
    "        # Compute dueling Q-values into out buffer\n",
    "        if out is not None:\n",
    "            # Check shape compatibility\n",
    "            expected_shape = (batch_size, seq_len, self.num_actions) if not single_step else (batch_size, self.num_actions)\n",
    "            if out.shape != expected_shape:\n",
    "                raise ValueError(f\"out tensor shape {out.shape} doesn't match expected shape {expected_shape}\")\n",
    "            \n",
    "            # Compute advantages - advantages.mean() in-place into out\n",
    "            advantages_mean = advantages.mean(dim=-1, keepdim=True)\n",
    "            torch.sub(advantages, advantages_mean, out=out)\n",
    "            out.add_(values) # Add values in-place\n",
    "            \n",
    "            # Apply value transform in-place\n",
    "            if hasattr(self, '_use_value_transform') and self._use_value_transform:\n",
    "                out.copy_(out)\n",
    "            \n",
    "            if single_step:\n",
    "                out.squeeze_(1) # In-place squeeze\n",
    "            q_values = out\n",
    "        else:\n",
    "            # Calculate into a new buffer\n",
    "            q_values = (values + (advantages - advantages.mean(dim=-1, keepdim=True)))\n",
    "            if single_step:\n",
    "                q_values = q_values.squeeze(1) # (B, num_actions)\n",
    "        \n",
    "        return q_values, new_hidden\n",
    "\n",
    "class RecurrentDQNAgent:\n",
    "    def __init__(self, network: nn.Module, device: torch.device, num_actions: int):\n",
    "        self.network = network.to(device)\n",
    "        self.device = device\n",
    "        self.num_actions = num_actions\n",
    "        self.reset_hidden_state()\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "        self.prev_action = 0 # Start with action 0\n",
    "        self.prev_reward = 0.0\n",
    "\n",
    "    def select_action(self, state_np, override_action=None):\n",
    "        # Select action with optional action override for epsilon-greedy.\n",
    "        state = torch.from_numpy(state_np).unsqueeze(0).float().to(self.device) / 255.0\n",
    "        prev_action = torch.tensor([self.prev_action], dtype=torch.int64, device=self.device)\n",
    "        prev_reward = torch.tensor([self.prev_reward], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values, new_hidden = self.network(state, prev_action, prev_reward, self.hidden_state)\n",
    "            greedy_action = int(q_values.argmax(dim=1).item())\n",
    "            \n",
    "        # Use override action if provided (for epsilon-greedy), otherwise greedy\n",
    "        action = override_action if override_action is not None else greedy_action\n",
    "        \n",
    "        # Always advance hidden state on the observation and set executed action\n",
    "        self.hidden_state = new_hidden\n",
    "        self.prev_action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_prev_reward(self, reward: float):\n",
    "        # Update the previous reward for next action selection\n",
    "        self.prev_reward = reward\n",
    "\n",
    "\n",
    "def _device():\n",
    "    return torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def preview_recurrent_model(\n",
    "    checkpoint_path: str,\n",
    "    num_episodes: int = 5,\n",
    "    render: bool = True,\n",
    "    epsilon: float = 0.0,\n",
    "    game_name: str = 'MsPacman',\n",
    "    turn_off_lstm: bool = False\n",
    "):\n",
    "    device = _device()\n",
    "    env_name = f\"{game_name}NoFrameskip-v4\"\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "\n",
    "    base_env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
    "    atari_env = AtariPreprocessing(\n",
    "        base_env,\n",
    "        frame_skip=4,\n",
    "        grayscale_obs=True,\n",
    "        scale_obs=False,\n",
    "        noop_max=30,\n",
    "        terminal_on_life_loss=False\n",
    "    )\n",
    "    env = FrameStack(atari_env, **{FS: 4})\n",
    "\n",
    "    obs_shape = env.observation_space.shape\n",
    "    n_actions = env.action_space.n\n",
    "    action_meanings = env.unwrapped.get_action_meanings()\n",
    "    action_counts = {i: 0 for i in range(n_actions)}\n",
    "\n",
    "    print(f\"Observation shape: {obs_shape}, Actions: {n_actions}\")\n",
    "    print(\"Action meanings:\", action_meanings)\n",
    "\n",
    "    net = RecurrentDuelingDQN(obs_shape, n_actions, turn_off_lstm=turn_off_lstm).to(device)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "        env.close()\n",
    "        return\n",
    "\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    agent = RecurrentDQNAgent(net, device, n_actions)\n",
    "\n",
    "    episode_rewards, episode_lengths, episode_lives = [], [], []\n",
    "\n",
    "    obs, info = env.reset(seed=100500)\n",
    "    s = np.array(obs, dtype=np.uint8)\n",
    "    lives = 3\n",
    "    no_lives = info['lives'] == 0\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "        if lives == 0:\n",
    "            obs, i = env.reset(seed=100500 + ep * 23917)\n",
    "            lives = i['lives']\n",
    "        done = False\n",
    "        total_reward, steps = 0.0, 0\n",
    "\n",
    "        print(f\"\\n=== Episode {ep} ===\")\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "                agent.select_action(s, override_action=a)\n",
    "            else:\n",
    "                a = agent.select_action(s)\n",
    "\n",
    "            action_counts[a] += 1\n",
    "            obs2, r, term, trunc, i = env.step(a)\n",
    "\n",
    "            agent.update_prev_reward(r)\n",
    "            if not no_lives and i[\"lives\"] == 0:\n",
    "                lives = i[\"lives\"]\n",
    "                print(\"Lost last life! Left:\", lives)\n",
    "                agent.reset_hidden_state()\n",
    "\n",
    "            s = np.array(obs2, dtype=np.uint8)\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            done = term or trunc\n",
    "\n",
    "            if steps % 200 == 0:\n",
    "                try:\n",
    "                    print(f\"Steps: {steps}, Lives: {lives}, Reward: {total_reward:.2f}\")\n",
    "                except Exception:\n",
    "                    print(f\"Steps: {steps}, Reward: {total_reward:.2f}\")\n",
    "\n",
    "        final_lives = lives\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        episode_lives.append(final_lives)\n",
    "\n",
    "        print(f\"Episode {ep} finished: Reward={total_reward:.2f}, \"\n",
    "              f\"Lives={final_lives}, Steps={steps}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"SUMMARY OVER {num_episodes} EPISODES\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Avg Reward: {np.mean(episode_rewards):.2f}, \"\n",
    "          f\"Max: {np.max(episode_rewards):.2f}, Min: {np.min(episode_rewards):.2f}\")\n",
    "    print(f\"Avg Steps: {np.mean(episode_lengths):.1f}\")\n",
    "\n",
    "    print(\"\\nAction Usage:\")\n",
    "    total_actions = sum(action_counts.values())\n",
    "    for i, cnt in action_counts.items():\n",
    "        pct = cnt / total_actions * 100 if total_actions else 0.0\n",
    "        print(f\"  {action_meanings[i]}: {cnt} ({pct:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"lengths\": episode_lengths,\n",
    "        \"lives\": episode_lives,\n",
    "        \"action_counts\": action_counts,\n",
    "    }\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=\"MsPacman\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir)\n",
    "             if f.endswith(\".pth\") and pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def find_specific_checkpoint(checkpoint_dir=\"checkpoints\", number=20400, pattern=\"MsPacman\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir)\n",
    "             if f.endswith(\".pth\") and pattern in f and (('ep' + str(number)) in f)]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Ms. Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_pacman/ALE_MsPacman-v5_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_pacman/ALE_MsPacman-v5_best_model.pth\n",
      "Observation shape: (4, 84, 84), Actions: 9\n",
      "Action meanings: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_99466/3473432877.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Steps: 200, Lives: 3, Reward: 3340.00\n",
      "Steps: 400, Lives: 3, Reward: 5230.00\n",
      "Steps: 600, Lives: 3, Reward: 7020.00\n",
      "Steps: 800, Lives: 3, Reward: 10341.00\n",
      "Steps: 1000, Lives: 3, Reward: 10511.00\n",
      "Steps: 1200, Lives: 3, Reward: 10691.00\n",
      "Steps: 1400, Lives: 3, Reward: 14441.00\n",
      "Steps: 1600, Lives: 3, Reward: 16601.00\n",
      "Steps: 1800, Lives: 3, Reward: 17531.00\n",
      "Steps: 2000, Lives: 3, Reward: 17791.00\n",
      "Steps: 2200, Lives: 3, Reward: 17901.00\n",
      "Steps: 2400, Lives: 3, Reward: 18601.00\n",
      "Steps: 2600, Lives: 3, Reward: 19581.00\n",
      "Steps: 2800, Lives: 3, Reward: 19821.00\n",
      "Lost last life! Left: 0\n",
      "Episode 1 finished: Reward=19821.00, Lives=0, Steps=2825\n",
      "\n",
      "========================================\n",
      "SUMMARY OVER 1 EPISODES\n",
      "========================================\n",
      "Avg Reward: 19821.00, Max: 19821.00, Min: 19821.00\n",
      "Avg Steps: 2825.0\n",
      "\n",
      "Action Usage:\n",
      "  NOOP: 42 (1.5%)\n",
      "  UP: 169 (6.0%)\n",
      "  RIGHT: 35 (1.2%)\n",
      "  LEFT: 626 (22.2%)\n",
      "  DOWN: 613 (21.7%)\n",
      "  UPRIGHT: 912 (32.3%)\n",
      "  UPLEFT: 140 (5.0%)\n",
      "  DOWNRIGHT: 272 (9.6%)\n",
      "  DOWNLEFT: 16 (0.6%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [19821.0],\n",
       " 'lengths': [2825],\n",
       " 'lives': [0],\n",
       " 'action_counts': {0: 42,\n",
       "  1: 169,\n",
       "  2: 35,\n",
       "  3: 626,\n",
       "  4: 613,\n",
       "  5: 912,\n",
       "  6: 140,\n",
       "  7: 272,\n",
       "  8: 16}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name = \"MsPacman\"\n",
    "latest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\n",
    "specific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\n",
    "checkpoint = specific or os.path.join(\"checkpoints_pacman\", f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_recurrent_model(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=1,\n",
    "    render=True,\n",
    "    epsilon=0.00,\n",
    "    game_name=game_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_space_invaders/ALE_SpaceInvaders-v5_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_space_invaders/ALE_SpaceInvaders-v5_best_model.pth\n",
      "Observation shape: (4, 84, 84), Actions: 6\n",
      "Action meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_40889/3473432877.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n",
      "2025-09-14 17:58:14.394 Python[40889:4674224] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-09-14 17:58:14.394 Python[40889:4674224] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Steps: 200, Lives: 3, Reward: 175.00\n",
      "Steps: 400, Lives: 3, Reward: 635.00\n",
      "Steps: 600, Lives: 3, Reward: 850.00\n",
      "Steps: 800, Lives: 3, Reward: 1150.00\n",
      "Steps: 1000, Lives: 3, Reward: 1460.00\n",
      "Steps: 1200, Lives: 3, Reward: 1735.00\n",
      "Steps: 1400, Lives: 3, Reward: 2090.00\n",
      "Steps: 1600, Lives: 3, Reward: 2460.00\n",
      "Steps: 1800, Lives: 3, Reward: 2950.00\n",
      "Lost last life! Left: 0\n",
      "Episode 1 finished: Reward=3220.00, Lives=0, Steps=1937\n",
      "\n",
      "========================================\n",
      "SUMMARY OVER 1 EPISODES\n",
      "========================================\n",
      "Avg Reward: 3220.00, Max: 3220.00, Min: 3220.00\n",
      "Avg Steps: 1937.0\n",
      "\n",
      "Action Usage:\n",
      "  NOOP: 122 (6.3%)\n",
      "  FIRE: 420 (21.7%)\n",
      "  RIGHT: 406 (21.0%)\n",
      "  LEFT: 313 (16.2%)\n",
      "  RIGHTFIRE: 396 (20.4%)\n",
      "  LEFTFIRE: 280 (14.5%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [3220.0],\n",
       " 'lengths': [1937],\n",
       " 'lives': [0],\n",
       " 'action_counts': {0: 122, 1: 420, 2: 406, 3: 313, 4: 396, 5: 280}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name = \"SpaceInvaders\"\n",
    "latest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\n",
    "specific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\n",
    "checkpoint = os.path.join(\"checkpoints_space_invaders\", f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_recurrent_model(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=1,\n",
    "    render=True,\n",
    "    epsilon=0.00,\n",
    "    game_name=game_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_pong/ALE_Pong-v5_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_pong/ALE_Pong-v5_best_model.pth\n",
      "Observation shape: (4, 84, 84), Actions: 6\n",
      "Action meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_40889/2622313231.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Steps: 200, Lives: 3, Reward: 2.00\n",
      "Steps: 400, Lives: 3, Reward: 5.00\n",
      "Steps: 600, Lives: 3, Reward: 7.00\n",
      "Steps: 800, Lives: 3, Reward: 10.00\n",
      "Steps: 1000, Lives: 3, Reward: 12.00\n",
      "Steps: 1200, Lives: 3, Reward: 15.00\n",
      "Steps: 1400, Lives: 3, Reward: 17.00\n",
      "Steps: 1600, Lives: 3, Reward: 20.00\n",
      "Episode 1 finished: Reward=21.00, Lives=3, Steps=1642\n",
      "\n",
      "========================================\n",
      "SUMMARY OVER 1 EPISODES\n",
      "========================================\n",
      "Avg Reward: 21.00, Max: 21.00, Min: 21.00\n",
      "Avg Steps: 1642.0\n",
      "\n",
      "Action Usage:\n",
      "  NOOP: 286 (17.4%)\n",
      "  FIRE: 79 (4.8%)\n",
      "  RIGHT: 320 (19.5%)\n",
      "  LEFT: 191 (11.6%)\n",
      "  RIGHTFIRE: 584 (35.6%)\n",
      "  LEFTFIRE: 182 (11.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [21.0],\n",
       " 'lengths': [1642],\n",
       " 'lives': [3],\n",
       " 'action_counts': {0: 286, 1: 79, 2: 320, 3: 191, 4: 584, 5: 182}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name = \"Pong\"\n",
    "latest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\n",
    "specific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\n",
    "checkpoint = specific or os.path.join(\"checkpoints_pong\", f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_recurrent_model(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=1,\n",
    "    render=True,\n",
    "    epsilon=0.00,\n",
    "    game_name=game_name,\n",
    "    turn_off_lstm=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: checkpoints_cartpole/CartPole-v1_best_model.pth\n",
      "Using device: mps\n",
      "Loading model from: checkpoints_cartpole/CartPole-v1_best_model.pth\n",
      "Observation dim: 4, Actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_36370/538669789.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=500.00, steps=500\n",
      "Episode 2: reward=500.00, steps=500\n",
      "Episode 3: reward=500.00, steps=500\n",
      "\n",
      "==== SUMMARY ====\n",
      "Avg Reward: 500.00 (max 500.00, min 500.00)\n",
      "Avg Steps: 500.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, policy_net: nn.Module, device: torch.device):\n",
    "        self.policy_net = policy_net.to(device).eval()\n",
    "        self.device = device\n",
    "    def predict(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            q_values = self.policy_net(obs_tensor)\n",
    "            action = int(q_values.argmax(dim=1).item())\n",
    "        return action, None\n",
    "\n",
    "def _device():\n",
    "    return (torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "            else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"))\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"checkpoints_cartpole\", pattern=\"CartPole-v1\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\") and pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def find_specific_checkpoint(checkpoint_dir=\"checkpoints_cartpole\", number=10000, pattern=\"CartPole-v1\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    tag = f\"ep{number}\"\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\") and pattern in f and tag in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, files[0])\n",
    "\n",
    "def preview_cartpole(\n",
    "    checkpoint_path: str = None,\n",
    "    num_episodes: int = 3,\n",
    "    render: bool = True,\n",
    "    epsilon: float = 0.0\n",
    "):\n",
    "    env_id = \"CartPole-v1\"\n",
    "    device = _device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        latest = find_latest_checkpoint(\"checkpoints_cartpole\", env_id)\n",
    "        best   = os.path.join(\"checkpoints_cartpole\", f\"{env_id}_best_model.pth\")\n",
    "        checkpoint_path = latest if latest is not None else best\n",
    "\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(\"Checkpoint not found.\")\n",
    "        return\n",
    "\n",
    "    env = gym.make(env_id, render_mode=\"human\" if render else None)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    print(f\"Observation dim: {state_dim}, Actions: {action_dim}\")\n",
    "\n",
    "    net = DQN(state_dim, action_dim).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "    agent = DQNAgent(net, device)\n",
    "\n",
    "    episode_rewards, episode_lengths = [], []\n",
    "\n",
    "    for ep in range(1, num_episodes+1):\n",
    "        state, _ = env.reset(seed=100500 + ep)\n",
    "        done, steps, total_reward = False, 0, 0.0\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action, _ = agent.predict(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Episode {ep}: reward={total_reward:.2f}, steps={steps}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\n==== SUMMARY ====\")\n",
    "    print(f\"Avg Reward: {np.mean(episode_rewards):.2f} \"\n",
    "          f\"(max {np.max(episode_rewards):.2f}, min {np.min(episode_rewards):.2f})\")\n",
    "    print(f\"Avg Steps: {np.mean(episode_lengths):.1f}\")\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "specific = None# find_specific_checkpoint(\"checkpoints_cartpole\", number=290, pattern=env_id)\n",
    "latest = None # find_latest_checkpoint(\"checkpoints_cartpole\", pattern=env_id)\n",
    "checkpoint = specific or latest or os.path.join(\"checkpoints_cartpole\", f\"{env_id}_best_model.pth\")\n",
    "print(f\"Using checkpoint: {checkpoint}\")\n",
    "\n",
    "preview_cartpole(\n",
    "    checkpoint_path=checkpoint,\n",
    "    num_episodes=3,\n",
    "    render=True,\n",
    "    epsilon=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import AsyncVectorEnv\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "try:\n",
    "    from gymnasium.wrappers import FrameStackObservation as FrameStack\n",
    "    FS = \"stack_size\"\n",
    "except ImportError:\n",
    "    from gymnasium.wrappers import FrameStack\n",
    "    FS = \"num_stack\"\n",
    "\n",
    "def _device():\n",
    "    return (torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "            else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"))\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"checkpoints\", game_name=\"MsPacman\"):\n",
    "    pattern = os.path.join(checkpoint_dir, f\"ALE_{game_name}-v5_ep*.pth\")\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        return None, 0\n",
    "    pairs = []\n",
    "    for f in files:\n",
    "        m = re.search(r\"_ep(\\d+)\\.pth$\", f)\n",
    "        if m: pairs.append((int(m.group(1)), f))\n",
    "    if not pairs:\n",
    "        return None, 0\n",
    "    pairs.sort(key=lambda x: x[0])\n",
    "    return pairs[-1][1], pairs[-1][0]\n",
    "\n",
    "def build_actor_state(agents, idx_list, device, hidden_size=512):\n",
    "    if not idx_list:\n",
    "        return None\n",
    "    h_list, c_list = [], []\n",
    "    for i in idx_list:\n",
    "        if agents[i].hidden_state is None:\n",
    "            h_list.append(torch.zeros(1, 1, hidden_size, device=device))\n",
    "            c_list.append(torch.zeros(1, 1, hidden_size, device=device))\n",
    "        else:\n",
    "            h, c = agents[i].hidden_state\n",
    "            h_list.append(h.detach())\n",
    "            c_list.append(c.detach())\n",
    "    h0 = torch.cat(h_list, dim=1)  # (1, k, H)\n",
    "    c0 = torch.cat(c_list, dim=1)  # (1, k, H)\n",
    "    return (h0, c0)\n",
    "\n",
    "def make_atari_env(game_name, frame_skip=4, stack=4, noop_max=30, terminal_on_life_loss=False):\n",
    "    \"\"\"Factory -> thunk for vector env creation.\"\"\"\n",
    "    def _thunk():\n",
    "        base = gym.make(f\"{game_name}NoFrameskip-v4\")\n",
    "        wrapped = AtariPreprocessing(\n",
    "            base,\n",
    "            frame_skip=frame_skip,\n",
    "            grayscale_obs=True,\n",
    "            scale_obs=False,\n",
    "            noop_max=noop_max,\n",
    "            terminal_on_life_loss=terminal_on_life_loss\n",
    "        )\n",
    "        wrapped = FrameStack(wrapped, **{FS: stack})\n",
    "        return wrapped\n",
    "    return _thunk\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_checkpoint_parallel(\n",
    "    checkpoint_path: str = None,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    game_name: str = \"MsPacman\",\n",
    "    num_episodes: int = 100,\n",
    "    num_envs: int = 32,\n",
    "    epsilon: float = 0.0,\n",
    "    seed: int = 100500,\n",
    "    frame_skip: int = 4,\n",
    "    stack: int = 4,\n",
    "    noop_max: int = 30,\n",
    "    terminal_on_life_loss: bool = False,\n",
    "    turn_off_lstm: bool = False,\n",
    "    hidden_size: int = 512,\n",
    "    context: str = \"fork\"\n",
    "):\n",
    "    assert num_episodes > 0 and num_envs > 0\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    device = _device()\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        latest, _ = find_latest_checkpoint(checkpoint_dir, game_name)\n",
    "        best = os.path.join(checkpoint_dir, f\"ALE_{game_name}-v5_best_model.pth\")\n",
    "        checkpoint_path = latest if latest is not None else best\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    # Build vector envs\n",
    "    env_fns = [make_atari_env(game_name, frame_skip, stack, noop_max, terminal_on_life_loss)\n",
    "               for _ in range(num_envs)]\n",
    "    try:\n",
    "        env = AsyncVectorEnv(env_fns, shared_memory=False, copy=False, context=context)\n",
    "    except TypeError:\n",
    "        import multiprocessing as mp\n",
    "        ctx = mp.get_context(context)\n",
    "        env = AsyncVectorEnv(env_fns, shared_memory=False, copy=False, ctx=ctx)\n",
    "\n",
    "    obs_space = env.single_observation_space\n",
    "    act_space = env.single_action_space\n",
    "    obs_shape = obs_space.shape\n",
    "    n_actions = act_space.n\n",
    "\n",
    "    net = RecurrentDuelingDQN(obs_shape, n_actions, lstm_hidden_size=hidden_size, turn_off_lstm=turn_off_lstm).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    agents = [RecurrentDQNAgent(net, device, n_actions) for _ in range(num_envs)]\n",
    "\n",
    "    obs, infos = env.reset(seed=seed)\n",
    "    try:\n",
    "        lives = np.array([info.get(\"lives\", 0) for info in infos], dtype=np.int32)\n",
    "    except Exception:\n",
    "        lives = np.zeros(num_envs, dtype=np.int32)\n",
    "\n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    action_counts = {i: 0 for i in range(n_actions)}\n",
    "\n",
    "    cur_returns = np.zeros(num_envs, dtype=np.float32)\n",
    "    cur_lengths = np.zeros(num_envs, dtype=np.int32)\n",
    "    episodes_done = 0\n",
    "\n",
    "    actor_eps = [epsilon] * num_envs\n",
    "\n",
    "    while episodes_done < num_episodes:\n",
    "        live_idx = [i for i in range(num_envs)]\n",
    "        obs_batch_u8 = torch.from_numpy(obs).to(device)\n",
    "        obs_batch = obs_batch_u8.float() / 255.0\n",
    "        prev_a = torch.tensor([agents[i].prev_action for i in live_idx], device=device)\n",
    "        prev_r = torch.tensor([agents[i].prev_reward for i in live_idx], device=device, dtype=torch.float32)\n",
    "        h_state = build_actor_state(agents, live_idx, device, hidden_size=hidden_size)\n",
    "\n",
    "        q, new_h = net(obs_batch, prev_a, prev_r, h_state)\n",
    "        greedy = q.argmax(dim=1).tolist()\n",
    "\n",
    "        actions = [0] * num_envs\n",
    "        for slot, i in enumerate(live_idx):\n",
    "            a = greedy[slot]\n",
    "            if np.random.random() < actor_eps[i]:\n",
    "                a = act_space.sample()\n",
    "            agents[i].hidden_state = (new_h[0][:, slot:slot+1], new_h[1][:, slot:slot+1])\n",
    "            agents[i].prev_action = a\n",
    "            actions[i] = a\n",
    "            action_counts[a] += 1\n",
    "\n",
    "        next_obs, rewards, terms, truncs, infos = env.step(actions)\n",
    "        dones = np.logical_or(terms, truncs)\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            agents[i].update_prev_reward(float(rewards[i]))\n",
    "            cur_returns[i] += float(rewards[i])\n",
    "            cur_lengths[i] += 1\n",
    "\n",
    "            try:\n",
    "                new_lives = infos[i].get(\"lives\", lives[i])\n",
    "                lives[i] = new_lives\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if np.any(dones):\n",
    "            done_idx = np.nonzero(dones)[0].tolist()\n",
    "            for i in done_idx:\n",
    "                ep_returns.append(cur_returns[i])\n",
    "                ep_lengths.append(cur_lengths[i])\n",
    "                episodes_done += 1\n",
    "\n",
    "                agents[i].reset_hidden_state()\n",
    "                cur_returns[i] = 0.0\n",
    "                cur_lengths[i] = 0\n",
    "\n",
    "            try:\n",
    "                reset_obs, reset_infos = env.reset_done()\n",
    "                for j, i in enumerate(done_idx):\n",
    "                    next_obs[i] = reset_obs[j]\n",
    "                    try:\n",
    "                        lives[i] = reset_infos[j].get(\"lives\", 0)\n",
    "                    except Exception:\n",
    "                        lives[i] = 0\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if episodes_done >= num_episodes:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    rewards_np = np.asarray(ep_returns, dtype=np.float32)[:num_episodes]\n",
    "    lengths_np = np.asarray(ep_lengths, dtype=np.int32)[:num_episodes]\n",
    "\n",
    "    result = {\n",
    "        \"episodes\": num_episodes,\n",
    "        \"mean_reward\": float(rewards_np.mean()) if len(rewards_np) else 0.0,\n",
    "        \"std_reward\": float(rewards_np.std()) if len(rewards_np) else 0.0,\n",
    "        \"min_reward\": float(rewards_np.min()) if len(rewards_np) else 0.0,\n",
    "        \"max_reward\": float(rewards_np.max()) if len(rewards_np) else 0.0,\n",
    "        \"mean_length\": float(lengths_np.mean()) if len(lengths_np) else 0.0,\n",
    "        \"rewards\": rewards_np.tolist(),\n",
    "        \"lengths\": lengths_np.tolist(),\n",
    "        \"action_counts\": action_counts,\n",
    "        \"checkpoint\": checkpoint_path,\n",
    "        \"game\": game_name,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"num_envs\": num_envs,\n",
    "        \"frame_skip\": frame_skip,\n",
    "        \"stack\": stack,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    print(\"\\n==== EVALUATION SUMMARY ====\")\n",
    "    print(f\"Game: {game_name} | Episodes: {num_episodes} | Envs: {num_envs} | Eps: {epsilon}\")\n",
    "    if len(rewards_np):\n",
    "        print(f\"Reward  avg {result['mean_reward']:.2f}  std {result['std_reward']:.2f}  \"\n",
    "              f\"min {result['min_reward']:.2f}  max {result['max_reward']:.2f}\")\n",
    "        print(f\"Length  avg {result['mean_length']:.1f}\")\n",
    "    print(\"Action usage:\", {k: v for k, v in action_counts.items() if v})\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Ms. Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_40889/3353353048.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== EVALUATION SUMMARY ====\n",
      "Game: MsPacman | Episodes: 100 | Envs: 32 | Eps: 0.0\n",
      "Reward  avg 14403.34  std 4396.58  min 9280.00  max 19821.00\n",
      "Length  avg 1990.0\n",
      "Action usage: {0: 3653, 1: 15307, 2: 5116, 3: 54313, 4: 49702, 5: 73023, 6: 16834, 7: 16197, 8: 2911}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ngame_name = \"MsPacman\"\\nlatest = find_latest_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name)\\nspecific = find_specific_checkpoint(checkpoint_dir=\"checkpoints\", pattern=game_name, number=747560)\\ncheckpoint = specific or os.path.join(\"checkpoints_pacman\", f\"ALE_{game_name}-v5_best_model.pth\")\\nprint(f\"Using checkpoint: {checkpoint}\")\\n\\npreview_recurrent_model(\\n    checkpoint_path=checkpoint,\\n    num_episodes=100,\\n    render=False,\\n    epsilon=0.00,\\n    game_name=game_name\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_game = \"MsPacman\"\n",
    "ckpt = os.path.join(\"checkpoints_pacman\", f\"ALE_{env_game}-v5_best_model.pth\")\n",
    "out = evaluate_checkpoint_parallel(\n",
    "    checkpoint_path=ckpt,\n",
    "    checkpoint_dir=\"checkpoints_pacman\",\n",
    "    game_name=env_game,\n",
    "    num_episodes=100,\n",
    "    num_envs=32,\n",
    "    epsilon=0.00,\n",
    "    seed=100500,\n",
    "    frame_skip=4,\n",
    "    stack=4,\n",
    "    noop_max=30,\n",
    "    terminal_on_life_loss=False,\n",
    "    turn_off_lstm=False,\n",
    "    hidden_size=512,\n",
    "    context=\"fork\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_40889/3353353048.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== EVALUATION SUMMARY ====\n",
      "Game: SpaceInvaders | Episodes: 100 | Envs: 32 | Eps: 0.0\n",
      "Reward  avg 3005.00  std 134.93  min 2820.00  max 3220.00\n",
      "Length  avg 1959.5\n",
      "Action usage: {0: 13786, 1: 56957, 2: 51806, 3: 37002, 4: 51709, 5: 35396}\n"
     ]
    }
   ],
   "source": [
    "env_game = \"SpaceInvaders\"\n",
    "ckpt = os.path.join(\"checkpoints_space_invaders\", f\"ALE_{env_game}-v5_best_model.pth\")\n",
    "out = evaluate_checkpoint_parallel(\n",
    "    checkpoint_path=ckpt,\n",
    "    checkpoint_dir=\"checkpoints_space_invaders\",\n",
    "    game_name=env_game,\n",
    "    num_episodes=100,\n",
    "    num_envs=32,\n",
    "    epsilon=0.00,\n",
    "    seed=100500,\n",
    "    frame_skip=4,\n",
    "    stack=4,\n",
    "    noop_max=30,\n",
    "    terminal_on_life_loss=False,\n",
    "    turn_off_lstm=False,\n",
    "    hidden_size=512,\n",
    "    context=\"fork\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/42zwl58n76v8hl24vdbpyg_80000gn/T/ipykernel_40889/3353353048.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== EVALUATION SUMMARY ====\n",
      "Game: Pong | Episodes: 100 | Envs: 32 | Eps: 0.0\n",
      "Reward  avg 21.00  std 0.00  min 21.00  max 21.00\n",
      "Length  avg 1642.7\n",
      "Action usage: {0: 36574, 1: 11303, 2: 37444, 3: 24168, 4: 72270, 5: 28321}\n"
     ]
    }
   ],
   "source": [
    "env_game = \"Pong\"\n",
    "ckpt = os.path.join(\"checkpoints_pong\", f\"ALE_{env_game}-v5_best_model.pth\")\n",
    "out = evaluate_checkpoint_parallel(\n",
    "    checkpoint_path=ckpt,\n",
    "    checkpoint_dir=\"checkpoints_pong\",\n",
    "    game_name=env_game,\n",
    "    num_episodes=100,\n",
    "    num_envs=32,\n",
    "    epsilon=0.00,\n",
    "    seed=100500,\n",
    "    frame_skip=4,\n",
    "    stack=4,\n",
    "    noop_max=30,\n",
    "    terminal_on_life_loss=False,\n",
    "    turn_off_lstm=True,\n",
    "    hidden_size=512,\n",
    "    context=\"fork\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
